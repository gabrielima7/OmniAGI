{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üß† OmniAGI - Google Colab Edition\n",
                "\n",
                "**Framework AGI Completo com GPU Acelerada**\n",
                "\n",
                "Este notebook funciona tanto no Colab quanto localmente.\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1Ô∏è‚É£ Detectar Ambiente e GPU"
            ],
            "metadata": {
                "id": "check_gpu"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Detectar se est√° no Colab\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "print(f\"üìç Ambiente: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
                "\n",
                "# Verificar GPU\n",
                "import subprocess\n",
                "try:\n",
                "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n",
                "                          capture_output=True, text=True)\n",
                "    if result.returncode == 0:\n",
                "        print(f\"‚úÖ GPU: {result.stdout.strip()}\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è nvidia-smi falhou\")\n",
                "except FileNotFoundError:\n",
                "    print(\"‚ö†Ô∏è nvidia-smi n√£o encontrado\")\n",
                "\n",
                "# PyTorch e CUDA\n",
                "import torch\n",
                "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
                "print(f\"‚úÖ CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    GPU_NAME = torch.cuda.get_device_name(0)\n",
                "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f\"‚úÖ GPU: {GPU_NAME}\")\n",
                "    print(f\"‚úÖ VRAM: {VRAM_GB:.1f} GB\")\n",
                "else:\n",
                "    VRAM_GB = 0\n",
                "    print(\"‚ö†Ô∏è Sem GPU - usando CPU\")"
            ],
            "metadata": {
                "id": "gpu_check"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2Ô∏è‚É£ Instalar Depend√™ncias (apenas Colab)"
            ],
            "metadata": {
                "id": "install"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "if IN_COLAB:\n",
                "    print(\"üì¶ Instalando depend√™ncias no Colab...\")\n",
                "    !pip install -q rwkv tokenizers structlog chromadb sentence-transformers gitpython beautifulsoup4 click\n",
                "    print(\"‚úÖ Depend√™ncias instaladas!\")\n",
                "else:\n",
                "    print(\"üìç Ambiente local - assumindo depend√™ncias j√° instaladas\")\n",
                "    print(\"   Se precisar instalar: pip install rwkv structlog chromadb sentence-transformers\")"
            ],
            "metadata": {
                "id": "install_deps"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3Ô∏è‚É£ Configurar OmniAGI"
            ],
            "metadata": {
                "id": "setup"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "if IN_COLAB:\n",
                "    # Colab: clonar do GitHub\n",
                "    if not os.path.exists('/content/OmniAGI'):\n",
                "        !git clone https://github.com/gabrielima7/OmniAGI.git /content/OmniAGI\n",
                "        print(\"‚úÖ Reposit√≥rio clonado!\")\n",
                "    else:\n",
                "        !cd /content/OmniAGI && git pull\n",
                "        print(\"‚úÖ Reposit√≥rio atualizado!\")\n",
                "    \n",
                "    OMNIAGI_PATH = '/content/OmniAGI'\n",
                "    os.chdir(OMNIAGI_PATH)\n",
                "    !mkdir -p models/rwkv\n",
                "else:\n",
                "    # Local: detectar caminho do notebook\n",
                "    notebook_dir = os.getcwd()\n",
                "    \n",
                "    # Se est√° em notebooks/, subir um n√≠vel\n",
                "    if notebook_dir.endswith('notebooks'):\n",
                "        OMNIAGI_PATH = os.path.dirname(notebook_dir)\n",
                "    else:\n",
                "        # Tentar encontrar o diret√≥rio OmniAGI\n",
                "        if os.path.exists(os.path.join(notebook_dir, 'omniagi')):\n",
                "            OMNIAGI_PATH = notebook_dir\n",
                "        elif os.path.exists(os.path.join(os.path.dirname(notebook_dir), 'omniagi')):\n",
                "            OMNIAGI_PATH = os.path.dirname(notebook_dir)\n",
                "        else:\n",
                "            OMNIAGI_PATH = '/media/zorin/HD/projetos/OmniAGI'\n",
                "    \n",
                "    os.chdir(OMNIAGI_PATH)\n",
                "    print(f\"‚úÖ Usando diret√≥rio local: {OMNIAGI_PATH}\")\n",
                "\n",
                "# Adicionar ao path\n",
                "if OMNIAGI_PATH not in sys.path:\n",
                "    sys.path.insert(0, OMNIAGI_PATH)\n",
                "\n",
                "print(f\"üìÅ Diret√≥rio: {os.getcwd()}\")\n",
                "print(f\"üìÅ Python path: {OMNIAGI_PATH}\")"
            ],
            "metadata": {
                "id": "setup_path"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4Ô∏è‚É£ Configurar Modelo RWKV"
            ],
            "metadata": {
                "id": "model_config"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import torch\n",
                "\n",
                "# Detectar VRAM\n",
                "if torch.cuda.is_available():\n",
                "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "else:\n",
                "    VRAM_GB = 0\n",
                "\n",
                "print(f\"üîç VRAM dispon√≠vel: {VRAM_GB:.1f} GB\")\n",
                "\n",
                "# Selecionar modelo e estrat√©gia baseado na VRAM\n",
                "if VRAM_GB >= 35:\n",
                "    MODEL_SIZE = \"7b\"\n",
                "    STRATEGY = \"cuda fp16\"\n",
                "    MODEL_URL = \"https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-7B-v3-20241112-ctx4096.pth\"\n",
                "elif VRAM_GB >= 14:\n",
                "    MODEL_SIZE = \"3b\"\n",
                "    STRATEGY = \"cuda fp16\"\n",
                "    MODEL_URL = \"https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-3B-v2.1-20240417-ctx4096.pth\"\n",
                "elif VRAM_GB >= 6:\n",
                "    MODEL_SIZE = \"1b6\"\n",
                "    STRATEGY = \"cuda fp16\"\n",
                "    MODEL_URL = \"https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth\"\n",
                "elif VRAM_GB >= 4:\n",
                "    MODEL_SIZE = \"1b6\"\n",
                "    STRATEGY = \"cuda fp16 -> cpu fp32\"  # H√≠brido\n",
                "    MODEL_URL = \"https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth\"\n",
                "else:\n",
                "    MODEL_SIZE = \"1b6\"\n",
                "    STRATEGY = \"cpu fp32\"\n",
                "    MODEL_URL = \"https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth\"\n",
                "\n",
                "# Verificar se modelo existe localmente\n",
                "MODEL_PATH = f\"models/rwkv/rwkv-6-{MODEL_SIZE}.pth\"\n",
                "\n",
                "# Tamb√©m verificar o modelo 3B que j√° pode existir\n",
                "if os.path.exists(\"models/rwkv/rwkv-6-3b.pth\") and VRAM_GB >= 4:\n",
                "    MODEL_PATH = \"models/rwkv/rwkv-6-3b.pth\"\n",
                "    MODEL_SIZE = \"3b\"\n",
                "    if VRAM_GB < 6:\n",
                "        STRATEGY = \"cuda fp16 -> cpu fp32\"\n",
                "\n",
                "print(f\"üì¶ Modelo selecionado: RWKV-6 {MODEL_SIZE.upper()}\")\n",
                "print(f\"‚öôÔ∏è Estrat√©gia: {STRATEGY}\")\n",
                "print(f\"üìÅ Caminho: {MODEL_PATH}\")\n",
                "print(f\"üìÅ Existe: {os.path.exists(MODEL_PATH)}\")"
            ],
            "metadata": {
                "id": "model_config_cell"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5Ô∏è‚É£ Download do Modelo (se necess√°rio)"
            ],
            "metadata": {
                "id": "download"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "\n",
                "if not os.path.exists(MODEL_PATH):\n",
                "    print(f\"‚¨áÔ∏è Baixando modelo {MODEL_SIZE.upper()}...\")\n",
                "    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
                "    \n",
                "    if IN_COLAB:\n",
                "        !wget -q --show-progress -O {MODEL_PATH} {MODEL_URL}\n",
                "    else:\n",
                "        import urllib.request\n",
                "        print(f\"   URL: {MODEL_URL}\")\n",
                "        print(\"   Isso pode demorar alguns minutos...\")\n",
                "        urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
                "    \n",
                "    print(f\"‚úÖ Modelo baixado!\")\n",
                "else:\n",
                "    print(f\"‚úÖ Modelo j√° existe: {MODEL_PATH}\")\n",
                "\n",
                "# Verificar tamanho\n",
                "if os.path.exists(MODEL_PATH):\n",
                "    size_gb = os.path.getsize(MODEL_PATH) / 1e9\n",
                "    print(f\"üìä Tamanho: {size_gb:.2f} GB\")"
            ],
            "metadata": {
                "id": "download_cell"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6Ô∏è‚É£ Carregar Modelo"
            ],
            "metadata": {
                "id": "load"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from rwkv.model import RWKV\n",
                "from rwkv.utils import PIPELINE, PIPELINE_ARGS\n",
                "\n",
                "print(f\"üîÑ Carregando RWKV-6 {MODEL_SIZE.upper()}...\")\n",
                "print(f\"   Estrat√©gia: {STRATEGY}\")\n",
                "\n",
                "model = RWKV(model=MODEL_PATH, strategy=STRATEGY)\n",
                "pipeline = PIPELINE(model, 'rwkv_vocab_v20230424')\n",
                "args = PIPELINE_ARGS(temperature=0.7, top_p=0.9)\n",
                "\n",
                "print(f\"‚úÖ Modelo carregado!\")\n",
                "\n",
                "# Mem√≥ria usada\n",
                "if torch.cuda.is_available():\n",
                "    used = torch.cuda.memory_allocated() / 1e9\n",
                "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f\"üìä VRAM usada: {used:.1f}/{total:.1f} GB\")"
            ],
            "metadata": {
                "id": "load_model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 7Ô∏è‚É£ Testar Gera√ß√£o"
            ],
            "metadata": {
                "id": "test_gen"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import time\n",
                "\n",
                "prompt = \"Artificial General Intelligence is\"\n",
                "print(f\"üìù Prompt: {prompt}\")\n",
                "\n",
                "start = time.time()\n",
                "output = pipeline.generate(prompt, token_count=50, args=args)\n",
                "elapsed = time.time() - start\n",
                "\n",
                "print(f\"\\nü§ñ Output: {output}\")\n",
                "print(f\"\\n‚ö° Tempo: {elapsed:.2f}s ({50/elapsed:.1f} tokens/s)\")"
            ],
            "metadata": {
                "id": "test_generation"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 8Ô∏è‚É£ Testar M√≥dulos OmniAGI"
            ],
            "metadata": {
                "id": "test_modules"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"üß™ TESTANDO M√ìDULOS OMNIAGI\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# 1. Consci√™ncia\n",
                "print(\"\\nüß† Consciousness Engine\")\n",
                "try:\n",
                "    from omniagi.consciousness import ConsciousnessEngine\n",
                "    c = ConsciousnessEngine()\n",
                "    c.awaken()\n",
                "    r = c.reflect()\n",
                "    print(f\"  State: {r['state']} ‚úÖ\")\n",
                "except Exception as e:\n",
                "    print(f\"  Erro: {e} ‚ùå\")\n",
                "\n",
                "# 2. Math Solver\n",
                "print(\"\\nüìê Math Solver\")\n",
                "try:\n",
                "    from omniagi.benchmarks.arc_solver import ChainOfThoughtSolver\n",
                "    solver = ChainOfThoughtSolver()\n",
                "    tests = [('sum', '123+456', '579'), ('triple', '33', '99'), ('square', '15', '225')]\n",
                "    for t, p, e in tests:\n",
                "        r = solver.solve(t, p)\n",
                "        status = \"‚úÖ\" if r.answer == e else \"‚ùå\"\n",
                "        print(f\"  {p} = {r.answer} {status}\")\n",
                "except Exception as e:\n",
                "    print(f\"  Erro: {e} ‚ùå\")\n",
                "\n",
                "# 3. RAG\n",
                "print(\"\\nüìö RAG System\")\n",
                "try:\n",
                "    from omniagi.memory.rag import RAGSystem\n",
                "    rag = RAGSystem('test_notebook')\n",
                "    rag.initialize()\n",
                "    rag.add_document('OmniAGI is an AGI framework')\n",
                "    results = rag.search('What is OmniAGI?')\n",
                "    print(f\"  Documents: {rag.get_stats()['documents']} ‚úÖ\")\n",
                "except Exception as e:\n",
                "    print(f\"  Erro: {e} ‚ùå\")\n",
                "\n",
                "# 4. MultiModal\n",
                "print(\"\\nüé® MultiModal\")\n",
                "try:\n",
                "    from omniagi.multimodal.lightweight import LightweightMultiModal\n",
                "    mm = LightweightMultiModal()\n",
                "    mm.initialize()\n",
                "    sim = mm.similarity(\"cat\", \"dog\")\n",
                "    print(f\"  cat-dog similarity: {sim:.3f} ‚úÖ\")\n",
                "except Exception as e:\n",
                "    print(f\"  Erro: {e} ‚ùå\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"‚úÖ Testes conclu√≠dos!\")"
            ],
            "metadata": {
                "id": "test_omniagi"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 9Ô∏è‚É£ Chat com OmniAGI"
            ],
            "metadata": {
                "id": "chat"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def chat_with_agi(prompt, max_tokens=100):\n",
                "    \"\"\"Chat com o modelo RWKV.\"\"\"\n",
                "    system = \"\"\"You are OmniAGI, an artificial general intelligence.\n",
                "Respond helpfully and thoughtfully.\n",
                "\n",
                "\"\"\"\n",
                "    full_prompt = system + f\"User: {prompt}\\nOmniAGI:\"\n",
                "    response = pipeline.generate(full_prompt, token_count=max_tokens, args=args)\n",
                "    if \"User:\" in response:\n",
                "        response = response.split(\"User:\")[0]\n",
                "    return response.strip()\n",
                "\n",
                "# Testar\n",
                "print(\"ü§ñ Chat com OmniAGI\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "questions = [\n",
                "    \"What are you?\",\n",
                "    \"Calculate 25 + 37\",\n",
                "]\n",
                "\n",
                "for q in questions:\n",
                "    print(f\"\\nüë§ User: {q}\")\n",
                "    response = chat_with_agi(q, max_tokens=80)\n",
                "    print(f\"ü§ñ OmniAGI: {response[:200]}\")"
            ],
            "metadata": {
                "id": "chat_cell"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîü Sua Pergunta"
            ],
            "metadata": {
                "id": "your_question"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ‚úèÔ∏è Digite sua pergunta aqui:\n",
                "sua_pergunta = \"What is consciousness?\"\n",
                "\n",
                "print(f\"üë§ Voc√™: {sua_pergunta}\")\n",
                "resposta = chat_with_agi(sua_pergunta, max_tokens=150)\n",
                "print(f\"\\nü§ñ OmniAGI: {resposta}\")"
            ],
            "metadata": {
                "id": "your_chat"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "\n",
                "**GitHub**: https://github.com/gabrielima7/OmniAGI\n",
                "\n",
                "üåü **100% Precision Achieved!** üåü"
            ],
            "metadata": {
                "id": "footer"
            }
        }
    ]
}