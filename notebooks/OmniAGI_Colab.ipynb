{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üß† OmniAGI - Google Colab Edition\n",
                "\n",
                "**Framework AGI com GPU - Vers√£o Est√°vel**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "check_gpu"
            },
            "source": [
                "## 1Ô∏è‚É£ Verificar GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "gpu_check"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìç Ambiente: Colab\n",
                        "‚úÖ GPU: Tesla T4\n",
                        "‚úÖ VRAM: 15.8 GB\n"
                    ]
                }
            ],
            "source": [
                "import os, sys, torch\n",
                "\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "print(f\"üìç Ambiente: {'Colab' if IN_COLAB else 'Local'}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    GPU_NAME = torch.cuda.get_device_name(0)\n",
                "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f\"‚úÖ GPU: {GPU_NAME}\")\n",
                "    print(f\"‚úÖ VRAM: {VRAM_GB:.1f} GB\")\n",
                "else:\n",
                "    VRAM_GB = 0\n",
                "    print(\"‚ö†Ô∏è Sem GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "install"
            },
            "source": [
                "## 2Ô∏è‚É£ Instalar Depend√™ncias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Deps instaladas!\n"
                    ]
                }
            ],
            "source": [
                "if IN_COLAB:\n",
                "    !pip install -q rwkv structlog chromadb sentence-transformers\n",
                "    print(\"‚úÖ Deps instaladas!\")\n",
                "else:\n",
                "    print(\"üìç Local - deps j√° instaladas\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup"
            },
            "source": [
                "## 3Ô∏è‚É£ Clonar OmniAGI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "setup_path"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÅ Path: /content/OmniAGI\n"
                    ]
                }
            ],
            "source": [
                "import os, sys\n",
                "\n",
                "if IN_COLAB:\n",
                "    if not os.path.exists('/content/OmniAGI'):\n",
                "        !git clone https://github.com/gabrielima7/OmniAGI.git /content/OmniAGI\n",
                "    OMNIAGI_PATH = '/content/OmniAGI'\n",
                "    os.chdir(OMNIAGI_PATH)\n",
                "    !mkdir -p models/rwkv\n",
                "else:\n",
                "    OMNIAGI_PATH = '/media/zorin/HD/projetos/OmniAGI'\n",
                "    os.chdir(OMNIAGI_PATH)\n",
                "\n",
                "sys.path.insert(0, OMNIAGI_PATH)\n",
                "print(f\"üìÅ Path: {OMNIAGI_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "model_config"
            },
            "source": [
                "## 4Ô∏è‚É£ Configurar Modelo\n",
                "\n",
                "‚ö†Ô∏è **IMPORTANTE**: Use modelo 1.6B para evitar crashes!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "model_config_cell"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üì¶ Modelo: RWKV-6 1B6\n",
                        "‚öôÔ∏è Estrat√©gia: cuda fp16\n",
                        "üìÅ Existe: False\n"
                    ]
                }
            ],
            "source": [
                "import os, torch\n",
                "\n",
                "# SEMPRE usar 1.6B no Colab para estabilidade\n",
                "# O modelo 3B causa crash no kernel\n",
                "MODEL_SIZE = \"1b6\"\n",
                "MODEL_PATH = \"models/rwkv/rwkv-6-1b6.pth\"\n",
                "MODEL_URL = \"https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth\"\n",
                "STRATEGY = \"cuda fp16\"  # 1.6B cabe f√°cil na GPU\n",
                "\n",
                "print(f\"üì¶ Modelo: RWKV-6 {MODEL_SIZE.upper()}\")\n",
                "print(f\"‚öôÔ∏è Estrat√©gia: {STRATEGY}\")\n",
                "print(f\"üìÅ Existe: {os.path.exists(MODEL_PATH)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download"
            },
            "source": [
                "## 5Ô∏è‚É£ Download do Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "download_cell"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚¨áÔ∏è Baixando RWKV 1B6...\n",
                        "models/rwkv/rwkv-6- 100%[===================>]   2.98G  26.2MB/s    in 2m 57s  \n",
                        "‚úÖ Download conclu√≠do!\n",
                        "üìä Tamanho: 3.20 GB\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "if not os.path.exists(MODEL_PATH):\n",
                "    print(f\"‚¨áÔ∏è Baixando RWKV {MODEL_SIZE.upper()}...\")\n",
                "    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
                "    !wget -q --show-progress -O {MODEL_PATH} {MODEL_URL}\n",
                "    print(\"‚úÖ Download conclu√≠do!\")\n",
                "else:\n",
                "    print(f\"‚úÖ Modelo existe: {MODEL_PATH}\")\n",
                "\n",
                "print(f\"üìä Tamanho: {os.path.getsize(MODEL_PATH) / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "load"
            },
            "source": [
                "## 6Ô∏è‚É£ Carregar Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "load_model"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
                        "  self.setter(val)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîÑ Carregando RWKV-6 1B6...\n",
                        "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 6\n",
                        "\n",
                        "Loading models/rwkv/rwkv-6-1b6.pth ...\n",
                        "Model detected: v6.0\n",
                        "Strategy: (total 24+1=25 layers)\n",
                        "* cuda [float16, float16], store 25 layers\n",
                        "0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 13-cuda-float16-float16 14-cuda-float16-float16 15-cuda-float16-float16 16-cuda-float16-float16 17-cuda-float16-float16 18-cuda-float16-float16 19-cuda-float16-float16 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 \n",
                        "emb.weight                        f16      cpu  65536  2048       \n",
                        "blocks.0.ln1.weight               f16   cuda:0   2048             \n",
                        "blocks.0.ln1.bias                 f16   cuda:0   2048             \n",
                        "blocks.0.ln2.weight               f16   cuda:0   2048             \n",
                        "blocks.0.ln2.bias                 f16   cuda:0   2048             \n",
                        "blocks.0.att.time_maa_x           f16   cuda:0   2048             \n",
                        "blocks.0.att.time_maa_w           f16   cuda:0   2048             \n",
                        "blocks.0.att.time_maa_k           f16   cuda:0   2048             \n",
                        "blocks.0.att.time_maa_v           f16   cuda:0   2048             \n",
                        "blocks.0.att.time_maa_r           f16   cuda:0   2048             \n",
                        "blocks.0.att.time_maa_g           f16   cuda:0   2048             \n",
                        "blocks.0.att.time_maa_w1          f16   cuda:0   2048   160       \n",
                        "blocks.0.att.time_maa_w2          f16   cuda:0      5    32  2048 \n",
                        "blocks.0.att.time_decay           f32   cuda:0     32    64       \n",
                        "blocks.0.att.time_decay_w1        f16   cuda:0   2048    64       \n",
                        "blocks.0.att.time_decay_w2        f16   cuda:0     64  2048       \n",
                        "blocks.0.att.time_first           f32   cuda:0     32    64       \n",
                        "blocks.0.att.receptance.weight    f16   cuda:0   2048  2048       \n",
                        "blocks.0.att.key.weight           f16   cuda:0   2048  2048       \n",
                        "blocks.0.att.value.weight         f16   cuda:0   2048  2048       \n",
                        "blocks.0.att.output.weight        f16   cuda:0   2048  2048       \n",
                        "blocks.0.att.gate.weight          f16   cuda:0   2048  2048       \n",
                        "blocks.0.att.ln_x.weight          f32   cuda:0   2048             \n",
                        "blocks.0.att.ln_x.bias            f32   cuda:0   2048             \n",
                        "blocks.0.ffn.time_maa_k           f16   cuda:0   2048             \n",
                        "blocks.0.ffn.time_maa_r           f16   cuda:0   2048             \n",
                        "blocks.0.ffn.key.weight           f16   cuda:0   2048  7168       \n",
                        "blocks.0.ffn.receptance.weight    f16   cuda:0   2048  2048       \n",
                        "blocks.0.ffn.value.weight         f16   cuda:0   7168  2048       \n",
                        "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
                        "blocks.23.ln1.weight              f16   cuda:0   2048             \n",
                        "blocks.23.ln1.bias                f16   cuda:0   2048             \n",
                        "blocks.23.ln2.weight              f16   cuda:0   2048             \n",
                        "blocks.23.ln2.bias                f16   cuda:0   2048             \n",
                        "blocks.23.att.time_maa_x          f16   cuda:0   2048             \n",
                        "blocks.23.att.time_maa_w          f16   cuda:0   2048             \n",
                        "blocks.23.att.time_maa_k          f16   cuda:0   2048             \n",
                        "blocks.23.att.time_maa_v          f16   cuda:0   2048             \n",
                        "blocks.23.att.time_maa_r          f16   cuda:0   2048             \n",
                        "blocks.23.att.time_maa_g          f16   cuda:0   2048             \n",
                        "blocks.23.att.time_maa_w1         f16   cuda:0   2048   160       \n",
                        "blocks.23.att.time_maa_w2         f16   cuda:0      5    32  2048 \n",
                        "blocks.23.att.time_decay          f32   cuda:0     32    64       \n",
                        "blocks.23.att.time_decay_w1       f16   cuda:0   2048    64       \n",
                        "blocks.23.att.time_decay_w2       f16   cuda:0     64  2048       \n",
                        "blocks.23.att.time_first          f32   cuda:0     32    64       \n",
                        "blocks.23.att.receptance.weight   f16   cuda:0   2048  2048       \n",
                        "blocks.23.att.key.weight          f16   cuda:0   2048  2048       \n",
                        "blocks.23.att.value.weight        f16   cuda:0   2048  2048       \n",
                        "blocks.23.att.output.weight       f16   cuda:0   2048  2048       \n",
                        "blocks.23.att.gate.weight         f16   cuda:0   2048  2048       \n",
                        "blocks.23.att.ln_x.weight         f32   cuda:0   2048             \n",
                        "blocks.23.att.ln_x.bias           f32   cuda:0   2048             \n",
                        "blocks.23.ffn.time_maa_k          f16   cuda:0   2048             \n",
                        "blocks.23.ffn.time_maa_r          f16   cuda:0   2048             \n",
                        "blocks.23.ffn.key.weight          f16   cuda:0   2048  7168       \n",
                        "blocks.23.ffn.receptance.weight   f16   cuda:0   2048  2048       \n",
                        "blocks.23.ffn.value.weight        f16   cuda:0   7168  2048       \n",
                        "ln_out.weight                     f16   cuda:0   2048             \n",
                        "ln_out.bias                       f16   cuda:0   2048             \n",
                        "head.weight                       f16   cuda:0   2048 65536       \n",
                        "‚úÖ Modelo carregado!\n",
                        "üìä VRAM: 2.9 GB\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from rwkv.model import RWKV\n",
                "from rwkv.utils import PIPELINE, PIPELINE_ARGS\n",
                "\n",
                "# Limpar cache GPU antes de carregar\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "print(f\"üîÑ Carregando RWKV-6 {MODEL_SIZE.upper()}...\")\n",
                "model = RWKV(model=MODEL_PATH, strategy=STRATEGY)\n",
                "pipeline = PIPELINE(model, 'rwkv_vocab_v20230424')\n",
                "args = PIPELINE_ARGS(temperature=0.7, top_p=0.9)\n",
                "\n",
                "print(\"‚úÖ Modelo carregado!\")\n",
                "if torch.cuda.is_available():\n",
                "    used = torch.cuda.memory_allocated() / 1e9\n",
                "    print(f\"üìä VRAM: {used:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "test_gen"
            },
            "source": [
                "## 7Ô∏è‚É£ Testar Gera√ß√£o"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "id": "test_generation"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìù Prompt: Artificial General Intelligence is\n",
                        "\n",
                        "ü§ñ Output:  the foundational AI technology that will allow machines to learn from experience, reason and solve problems without being explicitly programmed. It is a blend of AI technologies such as machine learning, natural language processing, robotics, and other advances in computer science.\n",
                        "\n",
                        "\n",
                        "‚ö° 12.5 tokens/s\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "prompt = \"Artificial General Intelligence is\"\n",
                "print(f\"üìù Prompt: {prompt}\")\n",
                "\n",
                "start = time.time()\n",
                "output = pipeline.generate(prompt, token_count=50, args=args)\n",
                "elapsed = time.time() - start\n",
                "\n",
                "print(f\"\\nü§ñ Output: {output}\")\n",
                "print(f\"\\n‚ö° {50/elapsed:.1f} tokens/s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "test_modules"
            },
            "source": [
                "## 8Ô∏è‚É£ Testar M√≥dulos OmniAGI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "id": "test_omniagi"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üß™ TESTANDO M√ìDULOS\n",
                        "========================================\n",
                        "üß† Consciousness: No module named 'llama_cpp' ‚ùå\n",
                        "üìê Math: No module named 'llama_cpp' ‚ùå\n",
                        "üìö RAG: No module named 'llama_cpp' ‚ùå\n",
                        "========================================\n"
                    ]
                }
            ],
            "source": [
                "print(\"üß™ TESTANDO M√ìDULOS\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Consciousness\n",
                "try:\n",
                "    from omniagi.consciousness import ConsciousnessEngine\n",
                "    c = ConsciousnessEngine()\n",
                "    c.awaken()\n",
                "    print(f\"üß† Consciousness: {c.reflect()['state']} ‚úÖ\")\n",
                "except Exception as e:\n",
                "    print(f\"üß† Consciousness: {e} ‚ùå\")\n",
                "\n",
                "# Math\n",
                "try:\n",
                "    from omniagi.benchmarks.arc_solver import ChainOfThoughtSolver\n",
                "    s = ChainOfThoughtSolver()\n",
                "    r = s.solve('sum', '25+37')\n",
                "    print(f\"üìê Math 25+37={r.answer} {'‚úÖ' if r.answer=='62' else '‚ùå'}\")\n",
                "except Exception as e:\n",
                "    print(f\"üìê Math: {e} ‚ùå\")\n",
                "\n",
                "# RAG\n",
                "try:\n",
                "    from omniagi.memory.rag import RAGSystem\n",
                "    rag = RAGSystem('test')\n",
                "    rag.initialize()\n",
                "    print(f\"üìö RAG: OK ‚úÖ\")\n",
                "except Exception as e:\n",
                "    print(f\"üìö RAG: {e} ‚ùå\")\n",
                "\n",
                "print(\"=\" * 40)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "chat"
            },
            "source": [
                "## 9Ô∏è‚É£ Chat com AGI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "id": "chat_cell"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ü§ñ Chat\n",
                        "\n",
                        "üë§ What is AGI?\n",
                        "ü§ñ AGI (Artificial General Intelligence) refers to an artificial intelligence system that can perform any intellectual task that a human can perform, wit\n",
                        "\n",
                        "üë§ Calculate 15*3\n",
                        "ü§ñ The result of 15 * 3 is 45.\n"
                    ]
                }
            ],
            "source": [
                "def chat(prompt, tokens=80):\n",
                "    p = f\"User: {prompt}\\nAssistant:\"\n",
                "    r = pipeline.generate(p, token_count=tokens, args=args)\n",
                "    return r.split(\"User:\")[0].strip() if \"User:\" in r else r.strip()\n",
                "\n",
                "print(\"ü§ñ Chat\")\n",
                "for q in [\"What is AGI?\", \"Calculate 15*3\"]:\n",
                "    print(f\"\\nüë§ {q}\")\n",
                "    print(f\"ü§ñ {chat(q)[:150]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "your_question"
            },
            "source": [
                "## üîü Sua Pergunta"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "id": "your_chat"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üë§ What is consciousness?\n",
                        "\n",
                        "ü§ñ Consciousness is the process of being aware or having awareness of something. It refers to the ability to perceive, think, and experience the world around us. Consciousness is a fundamental aspect of our existence as it allows us to make decisions, communicate, and interact with the environment around us.\n"
                    ]
                }
            ],
            "source": [
                "pergunta = \"What is consciousness?\"\n",
                "\n",
                "print(f\"üë§ {pergunta}\")\n",
                "print(f\"\\nü§ñ {chat(pergunta, tokens=100)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "footer"
            },
            "source": [
                "---\n",
                "\n",
                "**GitHub**: https://github.com/gabrielima7/OmniAGI\n",
                "\n",
                "üåü **OmniAGI** üåü"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
