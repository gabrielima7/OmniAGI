{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üß† OmniAGI - Google Colab Edition\n",
                "\n",
                "**Framework AGI com GPU - Vers√£o Est√°vel**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "check_gpu"
            },
            "source": [
                "## 1Ô∏è‚É£ Verificar GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "gpu_check"
            },
            "outputs": [],
            "source": [
                "import os, sys, torch\n",
                "\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "print(f\"üìç Ambiente: {'Colab' if IN_COLAB else 'Local'}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    GPU_NAME = torch.cuda.get_device_name(0)\n",
                "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f\"‚úÖ GPU: {GPU_NAME}\")\n",
                "    print(f\"‚úÖ VRAM: {VRAM_GB:.1f} GB\")\n",
                "else:\n",
                "    VRAM_GB = 0\n",
                "    print(\"‚ö†Ô∏è Sem GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "install"
            },
            "source": [
                "## 2Ô∏è‚É£ Instalar Depend√™ncias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "if IN_COLAB:\n",
                "    !pip install -q rwkv structlog chromadb sentence-transformers\n",
                "    print(\"‚úÖ Deps instaladas!\")\n",
                "else:\n",
                "    print(\"üìç Local - deps j√° instaladas\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup"
            },
            "source": [
                "## 3Ô∏è‚É£ Clonar OmniAGI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup_path"
            },
            "outputs": [],
            "source": [
                "import os, sys\n",
                "\n",
                "if IN_COLAB:\n",
                "    if not os.path.exists('/content/OmniAGI'):\n",
                "        !git clone https://github.com/gabrielima7/OmniAGI.git /content/OmniAGI\n",
                "    OMNIAGI_PATH = '/content/OmniAGI'\n",
                "    os.chdir(OMNIAGI_PATH)\n",
                "    !mkdir -p models/rwkv\n",
                "else:\n",
                "    OMNIAGI_PATH = '/media/zorin/HD/projetos/OmniAGI'\n",
                "    os.chdir(OMNIAGI_PATH)\n",
                "\n",
                "sys.path.insert(0, OMNIAGI_PATH)\n",
                "print(f\"üìÅ Path: {OMNIAGI_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "model_config"
            },
            "source": [
                "## 4Ô∏è‚É£ Configurar Modelo\n",
                "\n",
                "‚ö†Ô∏è **IMPORTANTE**: Use modelo 1.6B para evitar crashes!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_config_cell"
            },
            "outputs": [],
            "source": [
                "import os, torch\n",
                "\n",
                "# SEMPRE usar 1.6B no Colab para estabilidade\n",
                "# O modelo 3B causa crash no kernel\n",
                "MODEL_SIZE = \"1b6\"\n",
                "MODEL_PATH = \"models/rwkv/rwkv-6-1b6.pth\"\n",
                "MODEL_URL = \"https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth\"\n",
                "STRATEGY = \"cuda fp16\"  # 1.6B cabe f√°cil na GPU\n",
                "\n",
                "print(f\"üì¶ Modelo: RWKV-6 {MODEL_SIZE.upper()}\")\n",
                "print(f\"‚öôÔ∏è Estrat√©gia: {STRATEGY}\")\n",
                "print(f\"üìÅ Existe: {os.path.exists(MODEL_PATH)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download"
            },
            "source": [
                "## 5Ô∏è‚É£ Download do Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_cell"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "if not os.path.exists(MODEL_PATH):\n",
                "    print(f\"‚¨áÔ∏è Baixando RWKV {MODEL_SIZE.upper()}...\")\n",
                "    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
                "    !wget -q --show-progress -O {MODEL_PATH} {MODEL_URL}\n",
                "    print(\"‚úÖ Download conclu√≠do!\")\n",
                "else:\n",
                "    print(f\"‚úÖ Modelo existe: {MODEL_PATH}\")\n",
                "\n",
                "print(f\"üìä Tamanho: {os.path.getsize(MODEL_PATH) / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "load"
            },
            "source": [
                "## 6Ô∏è‚É£ Carregar Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_model"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "from rwkv.model import RWKV\n",
                "from rwkv.utils import PIPELINE, PIPELINE_ARGS\n",
                "\n",
                "# Limpar cache GPU antes de carregar\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "print(f\"üîÑ Carregando RWKV-6 {MODEL_SIZE.upper()}...\")\n",
                "model = RWKV(model=MODEL_PATH, strategy=STRATEGY)\n",
                "pipeline = PIPELINE(model, 'rwkv_vocab_v20230424')\n",
                "args = PIPELINE_ARGS(temperature=0.7, top_p=0.9)\n",
                "\n",
                "print(\"‚úÖ Modelo carregado!\")\n",
                "if torch.cuda.is_available():\n",
                "    used = torch.cuda.memory_allocated() / 1e9\n",
                "    print(f\"üìä VRAM: {used:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "test_gen"
            },
            "source": [
                "## 7Ô∏è‚É£ Testar Gera√ß√£o"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_generation"
            },
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "prompt = \"Artificial General Intelligence is\"\n",
                "print(f\"üìù Prompt: {prompt}\")\n",
                "\n",
                "start = time.time()\n",
                "output = pipeline.generate(prompt, token_count=50, args=args)\n",
                "elapsed = time.time() - start\n",
                "\n",
                "print(f\"\\nü§ñ Output: {output}\")\n",
                "print(f\"\\n‚ö° {50/elapsed:.1f} tokens/s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "test_modules"
            },
            "source": [
                "## 8Ô∏è‚É£ Testar M√≥dulos OmniAGI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_omniagi"
            },
            "outputs": [],
            "source": [
                "print(\"üß™ TESTANDO M√ìDULOS\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Consciousness\n",
                "try:\n",
                "    from omniagi.consciousness import ConsciousnessEngine\n",
                "    c = ConsciousnessEngine()\n",
                "    c.awaken()\n",
                "    print(f\"üß† Consciousness: {c.reflect()['state']} ‚úÖ\")\n",
                "except Exception as e:\n",
                "    print(f\"üß† Consciousness: {e} ‚ùå\")\n",
                "\n",
                "# Math\n",
                "try:\n",
                "    from omniagi.benchmarks.arc_solver import ChainOfThoughtSolver\n",
                "    s = ChainOfThoughtSolver()\n",
                "    r = s.solve('sum', '25+37')\n",
                "    print(f\"üìê Math 25+37={r.answer} {'‚úÖ' if r.answer=='62' else '‚ùå'}\")\n",
                "except Exception as e:\n",
                "    print(f\"üìê Math: {e} ‚ùå\")\n",
                "\n",
                "# RAG\n",
                "try:\n",
                "    from omniagi.memory.rag import RAGSystem\n",
                "    rag = RAGSystem('test')\n",
                "    rag.initialize()\n",
                "    print(f\"üìö RAG: OK ‚úÖ\")\n",
                "except Exception as e:\n",
                "    print(f\"üìö RAG: {e} ‚ùå\")\n",
                "\n",
                "print(\"=\" * 40)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "chat"
            },
            "source": [
                "## 9Ô∏è‚É£ Chat com AGI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "chat_cell"
            },
            "outputs": [],
            "source": [
                "def chat(prompt, tokens=80):\n",
                "    p = f\"User: {prompt}\\nAssistant:\"\n",
                "    r = pipeline.generate(p, token_count=tokens, args=args)\n",
                "    return r.split(\"User:\")[0].strip() if \"User:\" in r else r.strip()\n",
                "\n",
                "print(\"ü§ñ Chat\")\n",
                "for q in [\"What is AGI?\", \"Calculate 15*3\"]:\n",
                "    print(f\"\\nüë§ {q}\")\n",
                "    print(f\"ü§ñ {chat(q)[:150]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "your_question"
            },
            "source": [
                "## üîü Sua Pergunta"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "your_chat"
            },
            "outputs": [],
            "source": [
                "pergunta = \"What is consciousness?\"\n",
                "\n",
                "print(f\"üë§ {pergunta}\")\n",
                "print(f\"\\nü§ñ {chat(pergunta, tokens=100)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "footer"
            },
            "source": [
                "---\n",
                "\n",
                "**GitHub**: https://github.com/gabrielima7/OmniAGI\n",
                "\n",
                "üåü **OmniAGI** üåü"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}